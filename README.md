# RL-Exploration-Paper-Lists

Paper Collection of Reinforcement Learning Exploration covers exploration of Muti-Arm-Bandit, Reinforcement Learning and Multi-agent Reinforcement Learning. 

Exploration Reinforcement Learning is an important topic in Reinforcement Learning research area, which is to essentially improve the sample efficiency in a MDP setting.

A simple form of exploration-exploitation dilemma can be seen from the Multi-Arm Bandit problems, and we include MAB papers because many theoretical idea can be drived from MAB studies. 

Recently, most DRL exploration researches are focus on sparse reward settings and the target is a little different with the former studies, however we still classify those methods based on their methodology.

Papers are sorted by time and classification. Any suggestions and pull requests are welcome.

The sharing principle of these references here is for research. If any authors do not want their paper to be listed here, please feel free to contact me (Email: ericliuof97 [AT] gmail.com).

## MAB exploration

## RL exploration

### Curiosity/Surprise Driven / Intrinsic Reward Methods

#### Review Papers

##### Intrinsic Reward Methods Review

* <Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010)> by Jürgen Schmidhuber, 2010.

* <What is intrinsic motivation? A typology of computational approaches> by Pierre-Yves Oudeyer and Frederic Kaplan, 2007.


#### Counts based methods

#### PAC-MDP

* <An empirical evaluation of interval estimation for markov decision processes> by Alexander L Strehl and Michael L Littman, 2004.

* <A theoretical analysis of model- based interval estimation.> by Alexander L Strehl and Michael L Littman, 2004.

* <R-max-a general polynomial time algorithm for near-optimal reinforcement learning> by Ronen I Brafman and Moshe Tennenholtz, 2002.

* <Near-optimal reinforcement learning in polynomial time> by Michael Kearns and Satinder Singh, 2002.

* <Efficient model-based exploration> by Marco Wiering and Jürgen Schmidhuber, 1998.


#### Beyesian Reinforcement Learning

* <Variational information maximisation for intrinsically motivated reinforcement learning> by Shakir Mohamed and Danilo Jimenez Rezende, 2015.

* <An information-theoretic approach to curiosity-driven reinforcement learning> by Susanne Still and Doina Precup, 2012.


#### Psudo Count

* <Unifying count-based exploration and intrinsic motivation> by Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos, 2016.

* <Count-based exploration with neural density models>by Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos, 2018

#### State Representation

* <# exploration: A study of count-based exploration for deep reinforcement learning> by Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel, 2017.



### Information theory based methods

#### Mutual Information

#### Information Gain

### Prediction Error based methods

## MARL exploration
